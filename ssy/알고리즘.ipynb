{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "알고리즘.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqUnwXQ3s4zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlewTscltFWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "a0fb9e5f-6820-4d8f-9124-9001abe277d7"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.32.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=504ac772596c13a8cb37ef5c7aaf8313ce32a0ad6a28c3c6135bc2fec9597690\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq5d7MwXtI-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -q install tensorflow-hub\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "CHANNELS = 3 # number of image channels (RGB)\n",
        "\n",
        "def build_graph(hub_module_url, target_image_path):\n",
        "  # Step 1) Prepare pre-trained model for extracting image features.\n",
        "  module = hub.Module(hub_module_url)\n",
        "  height, width = hub.get_expected_image_size(module)\n",
        "\n",
        "  # Copied a method of https://github.com/GoogleCloudPlatform/cloudml-samples/blob/bf0680726/flowers/trainer/model.py#L181\n",
        "  # and fixed for all type images (not only jpeg)\n",
        "  def decode_and_resize(image_str_tensor):\n",
        "    \"\"\"Decodes jpeg string, resizes it and returns a uint8 tensor.\"\"\"\n",
        "    image = tf.image.decode_image(image_str_tensor, channels=CHANNELS)\n",
        "    # Note resize expects a batch_size, but tf_map supresses that index,\n",
        "    # thus we have to expand then squeeze.  Resize returns float32 in the\n",
        "    # range [0, uint8_max]\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    image = tf.image.resize_bilinear(\n",
        "        image, [height, width], align_corners=False)\n",
        "    image = tf.squeeze(image, squeeze_dims=[0])\n",
        "    image = tf.cast(image, dtype=tf.uint8)\n",
        "    return image\n",
        "\n",
        "  def to_img_feature(images):\n",
        "    \"\"\"Extract the feature of image vectors\"\"\"\n",
        "    outputs = module(dict(images=images), signature=\"image_feature_vector\", as_dict=True)\n",
        "    return outputs['default']\n",
        "\n",
        "  # Step 2) Extract image features of the target image.\n",
        "  target_image_bytes = tf.gfile.GFile(target_image_path, 'rb').read()\n",
        "  target_image = tf.constant(target_image_bytes, dtype=tf.string)\n",
        "  target_image = decode_and_resize(target_image)\n",
        "  target_image = tf.image.convert_image_dtype(target_image, dtype=tf.float32)\n",
        "  target_image = tf.expand_dims(target_image, 0)\n",
        "  target_image = to_img_feature(target_image)\n",
        "\n",
        "  # Step 3) Extract image features of input images.\n",
        "  input_byte = tf.placeholder(tf.string, shape=[None])\n",
        "  input_image = tf.map_fn(decode_and_resize, input_byte, back_prop=False, dtype=tf.uint8)\n",
        "  input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
        "  input_image = to_img_feature(input_image)\n",
        "\n",
        "  # Step 4) Compare cosine_similarities of the target image and the input images.\n",
        "  dot = tf.tensordot(target_image, tf.transpose(input_image), 1)\n",
        "  similarity = dot / (tf.norm(target_image, axis=1) * tf.norm(input_image, axis=1))\n",
        "  similarity = tf.reshape(similarity, [-1])\n",
        "  \n",
        "  return input_byte, similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeLwx2LEtEYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "bc39d847-633e-4f38-d9bf-26660ae73cc5"
      },
      "source": [
        "# df = pd.read_csv('/content/drive/My Drive/W_combined_top.csv')\n",
        "df = pd.read_csv('/content/W_bottom_combined.csv')\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>gender</th>\n",
              "      <th>type</th>\n",
              "      <th>images</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://smartstore.naver.com/ether_kr/products...</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>https://shop-phinf.pstatic.net/20200302_8/1583...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://smartstore.naver.com/ether_kr/products...</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>https://shop-phinf.pstatic.net/20200302_270/15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://smartstore.naver.com/ether_kr/products...</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>https://shop-phinf.pstatic.net/20191125_55/157...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://smartstore.naver.com/ether_kr/products...</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>https://shop-phinf.pstatic.net/20191125_69/157...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://smartstore.naver.com/everydayworkout/p...</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>https://shop-phinf.pstatic.net/20200914_19/160...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>https://kr.puma.com/classics-3-4-culotte.html</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>http://d8wtay8lm19fy.cloudfront.net/puma_produ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449</th>\n",
              "      <td>https://kr.puma.com/knit-tricot-pant.html</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>http://d8wtay8lm19fy.cloudfront.net/puma_produ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>https://kr.puma.com/puma-x-hh-tfs-track-pant.html</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>http://d8wtay8lm19fy.cloudfront.net/puma_produ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>https://kr.puma.com/puma-x-hh-tfs-track-pant-8...</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>http://d8wtay8lm19fy.cloudfront.net/puma_produ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>https://kr.puma.com/tfs-og-retro-pants.html</td>\n",
              "      <td>W</td>\n",
              "      <td>BOTTOM</td>\n",
              "      <td>http://d8wtay8lm19fy.cloudfront.net/puma_produ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1453 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   link  ...                                             images\n",
              "0     https://smartstore.naver.com/ether_kr/products...  ...  https://shop-phinf.pstatic.net/20200302_8/1583...\n",
              "1     https://smartstore.naver.com/ether_kr/products...  ...  https://shop-phinf.pstatic.net/20200302_270/15...\n",
              "2     https://smartstore.naver.com/ether_kr/products...  ...  https://shop-phinf.pstatic.net/20191125_55/157...\n",
              "3     https://smartstore.naver.com/ether_kr/products...  ...  https://shop-phinf.pstatic.net/20191125_69/157...\n",
              "4     https://smartstore.naver.com/everydayworkout/p...  ...  https://shop-phinf.pstatic.net/20200914_19/160...\n",
              "...                                                 ...  ...                                                ...\n",
              "1448      https://kr.puma.com/classics-3-4-culotte.html  ...  http://d8wtay8lm19fy.cloudfront.net/puma_produ...\n",
              "1449          https://kr.puma.com/knit-tricot-pant.html  ...  http://d8wtay8lm19fy.cloudfront.net/puma_produ...\n",
              "1450  https://kr.puma.com/puma-x-hh-tfs-track-pant.html  ...  http://d8wtay8lm19fy.cloudfront.net/puma_produ...\n",
              "1451  https://kr.puma.com/puma-x-hh-tfs-track-pant-8...  ...  http://d8wtay8lm19fy.cloudfront.net/puma_produ...\n",
              "1452        https://kr.puma.com/tfs-og-retro-pants.html  ...  http://d8wtay8lm19fy.cloudfront.net/puma_produ...\n",
              "\n",
              "[1453 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPhdigfifCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이미지 저장할 파일 생성\n",
        "import os\n",
        "dir = '/content/W_bottom_input/'\n",
        "if not(os.path.isdir(dir)):\n",
        "  os.makedirs(os.path.join(dir))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ga_VyuZtLmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이미지 저장 -> 한 번 저장 후 주석처리!\n",
        "input_image_urls = []\n",
        "\n",
        "# for i in random.sample(range(0, 1453), 200):\n",
        "for i in range(len(df)):\n",
        "  globals()['input_image{}_url'.format(i)] = str(df.iloc[i,-1])\n",
        "  input_image_urls.append(globals()['input_image{}_url'.format(i)])\n",
        "\n",
        "input_img_paths = []\n",
        "\n",
        "for i, url in enumerate(input_image_urls):\n",
        "  if len(url) > 0:\n",
        "    path = \"/content/W_bottom_input/input_img%d.jpg\" % i\n",
        "    # !wget -q {url} -O {path}\n",
        "    !curl {url} > {path}\n",
        "    input_img_paths.append(path)\n",
        "\n",
        "# 타겟 설정\n",
        "target = random.randint(len(df))\n",
        "target_img_path = \"/content/W_bottom_input/input_img%d.jpg\" % target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8XyOC9RtqQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# 이미 유사도 측정된 경우(생성되어있는 유사도 csv사용)\n",
        "file = f'/content/{target}_similarity.csv'\n",
        "if os.path.isfile(file):\n",
        "  print('----exist-----')\n",
        "  df_result = pd.read_csv(file)\n",
        "  # Display results\n",
        "  print(\"# Target image\")\n",
        "  display(Image(target_img_path))\n",
        "  print(\"- similarity: %.2f\" % similarities[0])\n",
        "  cnt = 0\n",
        "  for index, line in df_result[:20].iterrows():\n",
        "    if line['similarity'] < 0.9999:\n",
        "      cnt += 1\n",
        "      # display(Image(line['path']))\n",
        "      print(\"- similarity: %.2f\" % line['similarity'])\n",
        "      # print(line['path'])\n",
        "      print('input:',str(line['path']).split('input_img')[-1].split('.')[0])\n",
        "      if cnt > 10:\n",
        "          break\n",
        "\n",
        "# 새롭게 유사도 csv 생성\n",
        "else:\n",
        "# Load bytes of image files\n",
        "  image_bytes = [tf.gfile.GFile(name, 'rb').read() \n",
        "                  for name in [target_img_path] + input_img_paths]\n",
        "\n",
        "  hub_module_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_96/feature_vector/1\"\n",
        "\n",
        "  with tf.Graph().as_default():\n",
        "    input_byte, similarity_op = build_graph(hub_module_url, target_img_path)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      t0 = time.time() # for time check\n",
        "      \n",
        "      # Inference similarities\n",
        "      similarities = sess.run(similarity_op, feed_dict={input_byte: image_bytes})\n",
        "      print(\"%d images inference time: %.2f s\" % (len(similarities), time.time() - t0))\n",
        "      \n",
        "      # result = similarities[1:]\n",
        "      # for idx, sim in enumerate(result):\n",
        "      #   overall.iloc[target, idx] = sim\n",
        "      df_result = pd.DataFrame(sorted(zip(similarities[1:], input_img_paths), reverse=True),columns=['similarity','path'])\n",
        "      df_result.to_csv(f'{target}_similarity.csv')\n",
        "\n",
        "      # Display results\n",
        "      print(\"# Target image\")\n",
        "      display(Image(target_img_path))\n",
        "      print(\"- similarity: %.2f\" % similarities[0])\n",
        "\n",
        "\n",
        "      print(\"# Input images\")\n",
        "      cnt = 0\n",
        "      for similarity, input_img_path in sorted(zip(similarities[1:], input_img_paths), reverse=True):\n",
        "        \n",
        "        if similarity < 0.9999:\n",
        "          cnt += 1\n",
        "          display(Image(input_img_path))\n",
        "          print(\"- similarity: %.2f\" % similarity)\n",
        "          print(input_img_path)\n",
        "\n",
        "        if cnt > 10: # TOP10까지만 보여주기\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}